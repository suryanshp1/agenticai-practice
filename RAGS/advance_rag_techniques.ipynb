{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f91c65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make sure you load the API keys for cloud providers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "407d7586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting the environment variables, the keys\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec99654",
   "metadata": {},
   "source": [
    "### Query Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8e0dc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Query: What are the benefits of using renewable energy sources?\n",
      "Expanded Queries: What advantages come from utilizing renewable energy sources?\n",
      "2. How does using renewable energy sources benefit us?\n",
      "3. What are the perks of relying on renewable energy sources for power?\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "expansion_template = \"\"\"Given the user question: {question}\n",
    "Generate three alternative versions that express the same information need but with different wording:\n",
    "1.\"\"\"\n",
    "\n",
    "expansion_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=expansion_template\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.7)\n",
    "\n",
    "expansion_chain = expansion_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Generate expanded queries\n",
    "original_query = \"What are the benefits of using renewable energy sources?\"\n",
    "expanded_queries = expansion_chain.invoke(original_query)\n",
    "\n",
    "print(\"Original Query:\", original_query)\n",
    "print(\"Expanded Queries:\", expanded_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b9e791",
   "metadata": {},
   "source": [
    "### Hypothetical Document Embeddings (HyDE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac6f5ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "\n",
    "loader = JSONLoader(\n",
    "    file_path=\"knowledge_base.json\",\n",
    "    jq_schema=\".[].content\",  # This extracts the content field from each array item\n",
    "    text_content=True\n",
    ")\n",
    "documents = loader.load()\n",
    "embedder = OpenAIEmbeddings()\n",
    "embeddings = embedder.embed_documents([doc.page_content for doc in documents])\n",
    "vector_db = FAISS.from_documents(documents, embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e00d5347",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# Create prompt for generating hypothetical document\n",
    "hyde_template = \"\"\"Based on the question: {question}\n",
    "Write a passage that could contain the answer to this question:\"\"\"\n",
    "\n",
    "hyde_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=hyde_template\n",
    ")\n",
    "llm = ChatOpenAI(temperature=0.2)\n",
    "hyde_chain = hyde_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Generate hypothetical document\n",
    "query = \"What dietary changes can reduce carbon footprint?\"\n",
    "hypothetical_doc = hyde_chain.invoke(query)\n",
    "\n",
    "# Use the hypothetical document for retrieval\n",
    "embeddings = OpenAIEmbeddings()\n",
    "embedded_query = embeddings.embed_query(hypothetical_doc)\n",
    "results = vector_db.similarity_search_by_vector(embedded_query, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac139545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='9b255915-fce3-4f56-ac1f-72972779109f', metadata={'source': '/Users/surajpandey/Documents/Python/agentic-ai-wrk/RAGS/knowledge_base.json', 'seq_num': 5}, page_content='Vector databases store high-dimensional vectors and efficiently perform similarity searches. Popular vector databases include Pinecone, Milvus, and FAISS. They use algorithms like HNSW (Hierarchical Navigable Small World) or IVF (Inverted File Index) to enable fast approximate nearest neighbor search. These databases are essential for scaling embedding-based retrieval systems to large document collections.'), Document(id='bac674b4-350d-4b87-bab8-6f12be93a0be', metadata={'source': '/Users/surajpandey/Documents/Python/agentic-ai-wrk/RAGS/knowledge_base.json', 'seq_num': 1}, page_content=\"Transformer models were introduced in the paper 'Attention Is All You Need' by Vaswani et al. in 2017. The architecture relies on self-attention mechanisms rather than recurrent or convolutional neural networks. This design allows for more parallelization during training and better handling of long-range dependencies in text.\"), Document(id='9b2e7062-0f58-4200-8b08-263cf55f8cc3', metadata={'source': '/Users/surajpandey/Documents/Python/agentic-ai-wrk/RAGS/knowledge_base.json', 'seq_num': 4}, page_content='Retrieval-Augmented Generation (RAG) combines a retrieval system with a text generator. The retriever fetches relevant documents from a knowledge base, and these documents are then provided as context to the generator. RAG models can be fine-tuned end-to-end and leverage large pre-trained models like BART or T5 for generation. This approach helps ground the generated text in factual information.')]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cef005",
   "metadata": {},
   "source": [
    "### Contextual Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ac9e935",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "# create basic retriever from the vector store\n",
    "base_retriever = vector_db.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_retriever=base_retriever,\n",
    "    base_compressor=compressor\n",
    ")\n",
    "compressed_doc = compression_retriever.invoke(\"How do transformers work?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "297c000c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/Users/surajpandey/Documents/Python/agentic-ai-wrk/RAGS/knowledge_base.json', 'seq_num': 1}, page_content=\"Transformer models were introduced in the paper 'Attention Is All You Need' by Vaswani et al. in 2017. The architecture relies on self-attention mechanisms rather than recurrent or convolutional neural networks. This design allows for more parallelization during training and better handling of long-range dependencies in text.\"),\n",
       " Document(metadata={'source': '/Users/surajpandey/Documents/Python/agentic-ai-wrk/RAGS/knowledge_base.json', 'seq_num': 2}, page_content='BERT (Bidirectional Encoder Representations from Transformers) was developed by Google AI Language team in 2018.')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce27778",
   "metadata": {},
   "source": [
    "### Maximum Marginal Relevance (MMR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e34fa75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='bac674b4-350d-4b87-bab8-6f12be93a0be', metadata={'source': '/Users/surajpandey/Documents/Python/agentic-ai-wrk/RAGS/knowledge_base.json', 'seq_num': 1}, page_content=\"Transformer models were introduced in the paper 'Attention Is All You Need' by Vaswani et al. in 2017. The architecture relies on self-attention mechanisms rather than recurrent or convolutional neural networks. This design allows for more parallelization during training and better handling of long-range dependencies in text.\"), Document(id='fc492055-f866-4ebb-9f23-0030caad05a8', metadata={'source': '/Users/surajpandey/Documents/Python/agentic-ai-wrk/RAGS/knowledge_base.json', 'seq_num': 3}, page_content='GPT (Generative Pre-trained Transformer) models are autoregressive language models that use transformer-based neural networks. Unlike BERT, which is bidirectional, GPT models are unidirectional and predict the next token based on previous tokens. The original GPT was introduced by OpenAI in 2018, followed by GPT-2 in 2019 and GPT-3 in 2020, each significantly larger than its predecessor.'), Document(id='9b255915-fce3-4f56-ac1f-72972779109f', metadata={'source': '/Users/surajpandey/Documents/Python/agentic-ai-wrk/RAGS/knowledge_base.json', 'seq_num': 5}, page_content='Vector databases store high-dimensional vectors and efficiently perform similarity searches. Popular vector databases include Pinecone, Milvus, and FAISS. They use algorithms like HNSW (Hierarchical Navigable Small World) or IVF (Inverted File Index) to enable fast approximate nearest neighbor search. These databases are essential for scaling embedding-based retrieval systems to large document collections.'), Document(id='9b2e7062-0f58-4200-8b08-263cf55f8cc3', metadata={'source': '/Users/surajpandey/Documents/Python/agentic-ai-wrk/RAGS/knowledge_base.json', 'seq_num': 4}, page_content='Retrieval-Augmented Generation (RAG) combines a retrieval system with a text generator. The retriever fetches relevant documents from a knowledge base, and these documents are then provided as context to the generator. RAG models can be fine-tuned end-to-end and leverage large pre-trained models like BART or T5 for generation. This approach helps ground the generated text in factual information.'), Document(id='201acb6a-429c-436e-baa3-4e11b044077c', metadata={'source': '/Users/surajpandey/Documents/Python/agentic-ai-wrk/RAGS/knowledge_base.json', 'seq_num': 2}, page_content='BERT (Bidirectional Encoder Representations from Transformers) was developed by Google AI Language team in 2018. It is pre-trained using masked language modeling and next sentence prediction tasks. BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers.')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vector_store = FAISS.from_documents(documents, embedder)\n",
    "\n",
    "mmr_results = vector_db.max_marginal_relevance_search(\n",
    "    query=\"What are transformer models?\",\n",
    "    k=5,\n",
    "    fetch_k=20,\n",
    "    lambda_mult=0.5\n",
    ")\n",
    "\n",
    "print(mmr_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2f4920",
   "metadata": {},
   "source": [
    "### Source Attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93a1ac4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Example documents\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"The transformer architecture was introduced in the paper 'Attention is All You Need' by Vaswani et al. in 2017.\",\n",
    "        metadata={\"source\": \"Neural Network Review 2021\", \"page\": 42}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"BERT uses bidirectional training of the Transformer, masked language modeling, and next sentence prediction tasks.\",\n",
    "        metadata={\"source\": \"Introduction to NLP\", \"page\": 137}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"GPT models are autoregressive transformers that predict the next token based on previous tokens.\",\n",
    "        metadata={\"source\": \"Large Language Models Survey\", \"page\": 89}\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39678a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector store and retriever\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vector_store = FAISS.from_documents(documents, embeddings)\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9468bb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source attribution prompt template\n",
    "\n",
    "attribution_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a precise AI assistant that provides well-sourced information.\n",
    "    Please ensure to cite your sources accurately.\n",
    "    Answer the following question based ONLY on the provided sources. for each fact or claim in your answer,\n",
    "    include citation using [1], [2], etc. that refers to the source. Include a numbered reference list at the end.\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Sources:\n",
    "    {sources}\n",
    "\n",
    "    Your Answer:\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57aa8cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a source-formatted string from documents\n",
    "def format_sources_with_citations(docs):\n",
    "    formatted_sources = []\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        source_info = f\"[{i}] {doc.metadata.get('source', 'Unknown source')}\"\n",
    "        if doc.metadata.get('page'):\n",
    "            source_info += f\", page {doc.metadata['page']}\"\n",
    "        formatted_sources.append(f\"{source_info}\\n{doc.page_content}\")\n",
    "    return \"\\n\\n\".join(formatted_sources)\n",
    "\n",
    "# Build the RAG chain with source attribution\n",
    "def generate_attributed_response(question):\n",
    "    # Retrieve relevant documents\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    \n",
    "    # Format sources with citation numbers\n",
    "    sources_formatted = format_sources_with_citations(retrieved_docs)\n",
    "    \n",
    "    # Create the attribution chain using LCEL\n",
    "    attribution_chain = (\n",
    "        attribution_prompt\n",
    "        | ChatOpenAI(temperature=0)\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    # Generate the response with citations\n",
    "    response = attribution_chain.invoke({\n",
    "        \"question\": question,\n",
    "        \"sources\": sources_formatted\n",
    "    })\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d688055b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer models work by utilizing self-attention mechanisms to weigh the importance of different input tokens when making predictions. This allows them to capture long-range dependencies in the data more effectively compared to traditional recurrent neural networks [1].\n",
      "\n",
      "One example of a transformer model is BERT (Bidirectional Encoder Representations from Transformers), which incorporates bidirectional training of the Transformer, masked language modeling, and next sentence prediction tasks to achieve state-of-the-art performance in various natural language processing tasks [2].\n",
      "\n",
      "Another example is the GPT (Generative Pre-trained Transformer) series of models, which are autoregressive transformers that predict the next token in a sequence based on the preceding tokens. This approach allows GPT models to generate coherent and contextually relevant text [3].\n",
      "\n",
      "Reference List:\n",
      "[1] Neural Network Review 2021, page 42\n",
      "[2] Introduction to NLP, page 137\n",
      "[3] Large Language Models Survey, page 89\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "question = \"How do transformer models work and what are some examples?\"\n",
    "attributed_answer = generate_attributed_response(question)\n",
    "print(attributed_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e11f23",
   "metadata": {},
   "source": [
    "### Self-consistency Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "936fc332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import List, Dict\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def verify_response_accuracy(\n",
    "    retrieved_docs: List[Document],\n",
    "    generated_answer: str,\n",
    "    llm: ChatOpenAI = None\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Verify if a generated answer is fully supported by the retrieved documents.\n",
    "    Args:\n",
    "        retrieved_docs: List of documents used to generate the answer\n",
    "        generated_answer: The answer produced by the RAG system\n",
    "        llm: Language model to use for verification\n",
    "    Returns:\n",
    "        Dictionary containing verification results and any identified issues\n",
    "    \"\"\"\n",
    "    if llm is None:\n",
    "        llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "        \n",
    "    # Create context from retrieved documents\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "    \n",
    "    # Define verification prompt - fixed to avoid JSON formatting issues in the template\n",
    "    verification_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    As a fact-checking assistant, verify whether the following answer is fully supported\n",
    "    by the provided context. Identify any statements that are not supported or contradict the context.\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Answer to verify:\n",
    "    {answer}\n",
    "    \n",
    "    Perform a detailed analysis with the following structure:\n",
    "    1. List any factual claims in the answer\n",
    "    2. For each claim, indicate whether it is:\n",
    "       - Fully supported (provide the supporting text from context)\n",
    "       - Partially supported (explain what parts lack support)\n",
    "       - Contradicted (identify the contradiction)\n",
    "       - Not mentioned in context\n",
    "    3. Overall assessment: Is the answer fully grounded in the context?\n",
    "    \n",
    "    Return your analysis in JSON format with the following structure:\n",
    "    {{\n",
    "      \"claims\": [\n",
    "        {{\n",
    "          \"claim\": \"The factual claim\",\n",
    "          \"status\": \"fully_supported|partially_supported|contradicted|not_mentioned\",\n",
    "          \"evidence\": \"Supporting or contradicting text from context\",\n",
    "          \"explanation\": \"Your explanation\"\n",
    "        }}\n",
    "      ],\n",
    "      \"fully_grounded\": true|false,\n",
    "      \"issues_identified\": [\"List any specific issues\"]\n",
    "    }}\n",
    "    \"\"\")\n",
    "    \n",
    "    # Create verification chain using LCEL\n",
    "    verification_chain = (\n",
    "        verification_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    # Run verification\n",
    "    result = verification_chain.invoke({\n",
    "        \"context\": context,\n",
    "        \"answer\": generated_answer\n",
    "    })\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "658d8d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"claims\": [\n",
      "        {\n",
      "            \"claim\": \"The transformer architecture was introduced by OpenAI in 2018\",\n",
      "            \"status\": \"contradicted\",\n",
      "            \"evidence\": \"The transformer architecture was introduced in the paper 'Attention Is All You Need' by Vaswani et al. in 2017.\",\n",
      "            \"explanation\": \"The claim is contradicted by the fact that the transformer architecture was actually introduced in 2017 by Vaswani et al., not by OpenAI in 2018.\"\n",
      "        },\n",
      "        {\n",
      "            \"claim\": \"The transformer architecture uses recurrent neural networks\",\n",
      "            \"status\": \"contradicted\",\n",
      "            \"evidence\": \"It relies on self-attention mechanisms instead of recurrent or convolutional neural networks.\",\n",
      "            \"explanation\": \"The claim is contradicted by the fact that the transformer architecture does not use recurrent neural networks but relies on self-attention mechanisms.\"\n",
      "        },\n",
      "        {\n",
      "            \"claim\": \"BERT is a transformer model developed by Google\",\n",
      "            \"status\": \"fully_supported\",\n",
      "            \"evidence\": \"BERT is a transformer-based model developed by Google that uses masked language modeling and next sentence prediction as pre-training objectives.\",\n",
      "            \"explanation\": \"This claim is fully supported by the provided context.\"\n",
      "        }\n",
      "    ],\n",
      "    \"fully_grounded\": false,\n",
      "    \"issues_identified\": [\"Incorrect attribution of the introduction of the transformer architecture and the incorrect statement about the use of recurrent neural networks in the transformer architecture.\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "retrieved_docs = [\n",
    "    Document(page_content=\"The transformer architecture was introduced in the paper 'Attention Is All You Need' by Vaswani et al. in 2017. It relies on self-attention mechanisms instead of recurrent or convolutional neural networks.\"),\n",
    "    Document(page_content=\"BERT is a transformer-based model developed by Google that uses masked language modeling and next sentence prediction as pre-training objectives.\")\n",
    "]\n",
    "\n",
    "generated_answer = \"The transformer architecture was introduced by OpenAI in 2018 and uses recurrent neural networks. BERT is a transformer model developed by Google.\"\n",
    "\n",
    "verification_result = verify_response_accuracy(retrieved_docs, generated_answer)\n",
    "print(verification_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23f458d",
   "metadata": {},
   "source": [
    "### Corrective RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db9108d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "class DocumentRelevanceScore(BaseModel):\n",
    "    \"\"\"Binary relevance score for document evaluation.\"\"\"\n",
    "    is_relevant: bool = Field(description=\"Whether the document contains information relevant to the query\")\n",
    "    reasoning: str = Field(description=\"Explanation for the relevance decision\")\n",
    "\n",
    "def evaluate_document(document, query, llm):\n",
    "    \"\"\"Evaluate if a document is relevant to a query.\"\"\"\n",
    "    prompt = f\"\"\" You are an expert document evaluator. Your task is to\n",
    "    determine if the following document contains information relevant to the\n",
    "    given query.\n",
    "    Query: {query}\n",
    "    Document content:\n",
    "    {document.page_content}\n",
    "    Analyze whether this document contains information that helps answer the\n",
    "    query.\n",
    "    \"\"\"\n",
    "    evaluation = llm.with_structured_output(DocumentRelevanceScore).invoke(prompt)\n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0599a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
