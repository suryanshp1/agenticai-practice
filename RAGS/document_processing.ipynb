{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3595c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/Users/surajpandey/Documents/Python/agentic-ai-wrk/RAGS/knowledge_base.json', 'seq_num': 1}, page_content=\"Transformer models were introduced in the paper 'Attention Is All You Need' by Vaswani et al. in 2017. The architecture relies on self-attention mechanisms rather than recurrent or convolutional neural networks. This design allows for more parallelization during training and better handling of long-range dependencies in text.\"),\n",
       " Document(metadata={'source': '/Users/surajpandey/Documents/Python/agentic-ai-wrk/RAGS/knowledge_base.json', 'seq_num': 2}, page_content='BERT (Bidirectional Encoder Representations from Transformers) was developed by Google AI Language team in 2018. It is pre-trained using masked language modeling and next sentence prediction tasks. BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers.'),\n",
       " Document(metadata={'source': '/Users/surajpandey/Documents/Python/agentic-ai-wrk/RAGS/knowledge_base.json', 'seq_num': 3}, page_content='GPT (Generative Pre-trained Transformer) models are autoregressive language models that use transformer-based neural networks. Unlike BERT, which is bidirectional, GPT models are unidirectional and predict the next token based on previous tokens. The original GPT was introduced by OpenAI in 2018, followed by GPT-2 in 2019 and GPT-3 in 2020, each significantly larger than its predecessor.'),\n",
       " Document(metadata={'source': '/Users/surajpandey/Documents/Python/agentic-ai-wrk/RAGS/knowledge_base.json', 'seq_num': 4}, page_content='Retrieval-Augmented Generation (RAG) combines a retrieval system with a text generator. The retriever fetches relevant documents from a knowledge base, and these documents are then provided as context to the generator. RAG models can be fine-tuned end-to-end and leverage large pre-trained models like BART or T5 for generation. This approach helps ground the generated text in factual information.'),\n",
       " Document(metadata={'source': '/Users/surajpandey/Documents/Python/agentic-ai-wrk/RAGS/knowledge_base.json', 'seq_num': 5}, page_content='Vector databases store high-dimensional vectors and efficiently perform similarity searches. Popular vector databases include Pinecone, Milvus, and FAISS. They use algorithms like HNSW (Hierarchical Navigable Small World) or IVF (Inverted File Index) to enable fast approximate nearest neighbor search. These databases are essential for scaling embedding-based retrieval systems to large document collections.')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "\n",
    "loader = JSONLoader(file_path=\"knowledge_base.json\", jq_schema=\".[].content\", text_content=True)\n",
    "documents = loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d872728",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "\n",
    "document = \"\"\"# Introduction to RAG\n",
    "Retrieval-Augmented Generation (RAG) combines retrieval systems with\n",
    "generative AI models.\n",
    "It helps address hallucinations by grounding responses in retrieved\n",
    "information.\n",
    "## Key Components\n",
    "RAG consists of several components:\n",
    "1. Document processing\n",
    "2. Vector embedding\n",
    "3. Retrieval\n",
    "4. Augmentation\n",
    "5. Generation\n",
    "### Document Processing\n",
    "This step involves loading and chunking documents appropriately.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5e7b593",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_splitter.split_text(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1943410d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Introduction to RAG\\nRetrieval-Augmented Generation (RAG) combines retrieval systems with\\ngenerative AI models.',\n",
       " 'It helps address hallucinations by grounding responses in retrieved\\ninformation.\\n## Key Components\\nRAG consists of several components:',\n",
       " '1. Document processing\\n2. Vector embedding\\n3. Retrieval\\n4. Augmentation\\n5. Generation\\n### Document Processing',\n",
       " 'This step involves loading and chunking documents appropriately.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca2e2e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantical chunking - chunk based on meaning of content\n",
    "\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "text_splitter = SemanticChunker(\n",
    "    embeddings=embeddings,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af6ad68c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Introduction to RAG\\nRetrieval-Augmented Generation (RAG) combines retrieval systems with\\ngenerative AI models. It helps address hallucinations by grounding responses in retrieved\\ninformation. ## Key Components\\nRAG consists of several components:\\n1. Document processing\\n2. Vector embedding\\n3. Retrieval\\n4.',\n",
       " 'Augmentation\\n5. Generation\\n### Document Processing\\nThis step involves loading and chunking documents appropriately. ']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0440de69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cn/rj2hj5wn5cb0cxqfzy04f8c80000gn/T/ipykernel_40627/3475046693.py:25: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  results = hybrid_retriever.get_relevant_documents(\"What is RAG?\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(id='41ae0a3d-e7af-47c1-89b4-3195c45888c6', metadata={'source': '/Users/surajpandey/Documents/Python/agentic-ai-wrk/RAGS/knowledge_base.json', 'seq_num': 4}, page_content='Retrieval-Augmented Generation (RAG) combines a retrieval system with a text generator. The retriever fetches relevant documents from a knowledge base, and these documents are then provided as context to the generator. RAG models can be fine-tuned end-to-end and leverage large pre-trained models like BART or T5 for generation. This approach helps ground the generated text in factual information.'),\n",
       " Document(id='b73170e3-ee75-44c5-80c1-bc256690e28b', metadata={'source': '/Users/surajpandey/Documents/Python/agentic-ai-wrk/RAGS/knowledge_base.json', 'seq_num': 2}, page_content='BERT (Bidirectional Encoder Representations from Transformers) was developed by Google AI Language team in 2018. It is pre-trained using masked language modeling and next sentence prediction tasks. BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers.'),\n",
       " Document(id='b921d65d-2740-42ea-b0a6-027d2b32eff4', metadata={'source': '/Users/surajpandey/Documents/Python/agentic-ai-wrk/RAGS/knowledge_base.json', 'seq_num': 1}, page_content=\"Transformer models were introduced in the paper 'Attention Is All You Need' by Vaswani et al. in 2017. The architecture relies on self-attention mechanisms rather than recurrent or convolutional neural networks. This design allows for more parallelization during training and better handling of long-range dependencies in text.\"),\n",
       " Document(metadata={'source': '/Users/surajpandey/Documents/Python/agentic-ai-wrk/RAGS/knowledge_base.json', 'seq_num': 3}, page_content='GPT (Generative Pre-trained Transformer) models are autoregressive language models that use transformer-based neural networks. Unlike BERT, which is bidirectional, GPT models are unidirectional and predict the next token based on previous tokens. The original GPT was introduced by OpenAI in 2018, followed by GPT-2 in 2019 and GPT-3 in 2020, each significantly larger than its predecessor.'),\n",
       " Document(metadata={'source': '/Users/surajpandey/Documents/Python/agentic-ai-wrk/RAGS/knowledge_base.json', 'seq_num': 5}, page_content='Vector databases store high-dimensional vectors and efficiently perform similarity searches. Popular vector databases include Pinecone, Milvus, and FAISS. They use algorithms like HNSW (Hierarchical Navigable Small World) or IVF (Inverted File Index) to enable fast approximate nearest neighbor search. These databases are essential for scaling embedding-based retrieval systems to large document collections.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hybrid retriever\n",
    "\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "# Initialize the embeddings model\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "\n",
    "vector_store = FAISS.from_documents(documents=documents, embedding=embeddings_model)\n",
    "\n",
    "# Setup semantic retriever\n",
    "vector_retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Setup lexical retriever\n",
    "bm25_retriever = BM25Retriever.from_documents(documents)\n",
    "bm25_retriever.k = 5\n",
    "\n",
    "# combine retrievers\n",
    "hybrid_retriever = EnsembleRetriever(\n",
    "    retrievers=[vector_retriever, bm25_retriever],\n",
    "    weights=[0.7, 0.3],  # Adjust weights as needed\n",
    ")\n",
    "\n",
    "results = hybrid_retriever.get_relevant_documents(\"What is RAG?\")\n",
    "\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
